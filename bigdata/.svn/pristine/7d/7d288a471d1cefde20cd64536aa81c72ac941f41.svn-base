#!/bin/sh
###############################################################################
##  Author        : sunxiaolong
##  Name          : export_fact_table.sh
##  Functions     : 增量导入生产数据库数据到hive表
##  description   : 需要传入的参数有：厂商编号、数据库名、表名
##  Revisions or Comments
##  VER        DATE        AUTHOR           DESCRIPTION
##---------  ----------  ---------------  ------------------------------------ 
##  1.0      2019-07-18  sunxiaolong        1. CREATED THIS SHELL.   
###############################################################################

set -x

. `dirname ${0}`/func_utils.sh
. `dirname ${0}`/parameter.sh

function USAGE(){
    echo "[ERROR] Please check the number of parameters passed in!"
    echo "How to use this shell script!"
    echo "Sample: bash export_fact_table.sh [PROJECT] [DB_SCHEMA] [TAB_NAME]"
}


if [ $# -ne 3 ];then
    USAGE
    exit 1;
fi

start_time=$(GET_CURTIME_STANDARD)

PROJECT=$1 
DB_NAME=$2
TAB_NAME=$3 


#获取DB参数信息
db_info=(`mysql -h${ETL_HOST} -P${ETL_PORT} -u${ETL_USER} -p${ETL_PASSWD} -Nse "SELECT DB_HOST,DB_TYPE,DB_PORT,USERNAME,PASSWD FROM ETL.SCHEMA_INFO WHERE PROJECT='${PROJECT}' AND IS_VALID = 'Y'"`)

#获取[TAB_NAME]参数信息
tab_info=(`mysql -h${ETL_HOST} -P${ETL_PORT} -u${ETL_USER} -p${ETL_PASSWD} -Nse "SELECT PRI_KEY_COLUMN,LAST_VALUE FROM ETL.TABLE_INFO WHERE PROJECT='${PROJECT}' AND SCHEMA_NAME='${DB_NAME}' AND TABLE_NAME='${TAB_NAME}' AND IS_VALID = 'Y'"`)

if [[ -z ${db_info} ]];then
   echo "[-] This DB information is not exists,please check table 'ETL.SCHEMA_INFO' value!"
   exit 2
elif [[ -z ${tab_info} ]];then
   echo "[-] This table information is not exists,please check table 'ETL.TABLE_INFO' value!"
   exit 3
else
   DB_HOST=${db_info[0]}            #数据库地址IP
   DB_TYPE=${db_info[1]}            #数据库类型
   DB_PORT=${db_info[2]}            #端口号
   USERNAME=${db_info[3]}           #用户名
   PASSWD=${db_info[4]}             #密码
   
   PRI_KEY_COLUMN=${tab_info[0]}    #表主键字段
   LAST_VALUE=${tab_info[1]}        #上次更新时的主键值
   
   JDBC_URL=$(get_jdbc_url $DB_TYPE $DB_HOST $DB_PORT $DB_NAME)
   
fi


#判断hive中是否存在库${PROJECT}_${DB_NAME}，不存在则创建 
hive -e "create database if not exists ${PROJECT}_${DB_NAME};"

#创建执行sqooo的日志目录
mkdir -p ${V_SHELL_LOGS}/sqoop

#获取本次更新到的PRI_KEY_COLUMN的max value
#get_key_sql="SELECT MAX(${PRI_KEY_COLUMN}) FROM ${DB_NAME}.${TAB_NAME}"
#max_key_value=$(bash $(dirname ${0})/db_executor.sh -t ${DB_TYPE} -h ${DB_HOST} -P ${DB_PORT} -n ${USERNAME} -p ${PASSWD} -q "${get_key_sql}")

#更新本次采集的截止PRI_KEY_COLUMN
#update_sql_last_value="UPDATE ETL.TABLE_INFO SET LAST_VALUE=${max_key_value} WHERE PROJECT='${PROJECT}' AND SCHEMA_NAME='${DB_NAME}' AND TABLE_NAME='${TAB_NAME}'"
#bash $(dirname ${0})/db_executor.sh -t mysql -h hadoop01 -P 3306 -n hadoop -p hadoop -q "${update_sql_last_value}"

#执行job任务
job_name=${PROJECT}_${DB_NAME}_${TAB_NAME}

exist_job=$(sqoop-job --list | grep ${PROJECT}_${DB_NAME}_${TAB_NAME} | awk '{print $NF}')

if [[ ${job_name} == ${exist_job} ]];then
   sqoop job --exec  ${job_name}  > ${V_SHELL_LOGS}/sqoop/${PROJECT}_${DB_NAME}.${TAB_NAME}.log 2>&1
else
    #创建密码并且上传hdfs
    echo -n ${PASSWD} > ${V_SHELL_TMP}/${PROJECT}_${DB_NAME}.pwd
    hadoop fs -rm /user/hadoop/passwd/${PROJECT}_${DB_NAME}.pwd
    hadoop fs -put ${V_SHELL_TMP}/${PROJECT}_${DB_NAME}.pwd  /user/hadoop/passwd/
    rm -rf ${V_SHELL_TMP}/${PROJECT}_${DB_NAME}.pwd
    
    # 创建job任务
    sqoop job --create ${PROJECT}_${DB_NAME}_${TAB_NAME} -- import \
    --connect ${JDBC_URL}  \
    --username ${USERNAME} \
    --password-file /user/hadoop/passwd/${PROJECT}_${DB_NAME}.pwd \
    --table ${TAB_NAME} \
    --null-string '\\N' \
    --null-non-string '\\N' \
    --fields-terminated-by '\001' \
    --hive-import \
    --hive-database ${PROJECT}_${DB_NAME} \
    --hive-table ${TAB_NAME} \
    --split-by ${PRI_KEY_COLUMN} \
    --incremental append \
    --check-column ${PRI_KEY_COLUMN} \
    --last-value 0 
	   
    # 执行job任务
    sqoop job --exec  ${job_name} > ${V_SHELL_LOGS}/sqoop/${PROJECT}_${DB_NAME}.${TAB_NAME}.log 2>&1	
	
#   is_success && echo ${TAB_NAME}
 
fi 

#获取执行状态
status=$(echo `is_success` | awk '{print $2}' | awk -F ']' '{print $1}')
echo "${TAB_NAME} ${status}"

#更新本次的主键值
if [[ ${status} == "SUCCESSED" ]]; then

    min_value=$(cat ${V_SHELL_LOGS}/sqoop/${PROJECT}_${DB_NAME}.${TAB_NAME}.log | grep "Lower bound value" | awk -F ":" '{print $NF}' | awk '{print $NF}')
    max_value=$(cat ${V_SHELL_LOGS}/sqoop/${PROJECT}_${DB_NAME}.${TAB_NAME}.log | grep "Upper bound value" | awk -F ":" '{print $NF}' | awk '{print $NF}')
    
	if [[ -n ${min_value} && -n ${max_value} ]]; then
	
	    update_min_sql="UPDATE ETL.TABLE_INFO SET LAST_VALUE=${min_value} WHERE PROJECT='${PROJECT}' AND SCHEMA_NAME='${DB_NAME}' AND TABLE_NAME='${TAB_NAME}'"
	    update_max_sql="UPDATE ETL.TABLE_INFO SET THIS_VALUE=${max_value} WHERE PROJECT='${PROJECT}' AND SCHEMA_NAME='${DB_NAME}' AND TABLE_NAME='${TAB_NAME}'"
		mysql -h${ETL_HOST} -P${ETL_PORT} -u${ETL_USER} -p${ETL_PASSWD} -Nse "${update_min_sql}"
		mysql -h${ETL_HOST} -P${ETL_PORT} -u${ETL_USER} -p${ETL_PASSWD} -Nse "${update_max_sql}"
		
    else 
	    echo "The table ${TAB_NAME} data unchanged from last-value"
    fi
fi



#获取错误原因
if [[ ${status} == "FAILED" ]]; then

    error=$(cat ${V_SHELL_LOGS}/sqoop/${PROJECT}_${DB_NAME}.${TAB_NAME}.log | grep -E "ERROR | error | Caused by")
	echo ${error}
else 
    error=""
fi
	
#把执行状态结果插入日志表 etl.collect_logs
insert_log_sql="INSERT INTO etl.collect_logs (collect_type,state,project,db_name,table_name,db_user,sys_user,error) VALUES ('part','${status}','${PROJECT}','${DB_NAME}','${TAB_NAME}','${USERNAME}','${USER}','${error}')"
mysql -h${ETL_HOST} -P${ETL_PORT} -u${ETL_USER} -p${ETL_PASSWD} -e "${insert_log_sql}"


#刷新impala元数据
impala-shell -q "INVALIDATE METADATA"

#删除自动生成的java文件
rm -rf ${TAB_NAME}.java

end_time=$(GET_CURTIME_STANDARD)

duration=$(DATETIME_DIF "${start_time}" "${end_time}")

echo "[+] Process is end, And total time is ${duration} seconds !"


